{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45aed788",
   "metadata": {},
   "source": [
    "### RLHF to check ACD works\n",
    "This notebook uses ACD (Adversarial Contrastive Distillation) a HELLASWAG inspired data generation to fit RLHF. It also uses a portion of the dataset to measure model performance improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76b6c5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Hugging Face Transformers & Datasets\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,    \n",
    "    AutoModelForSequenceClassification,\n",
    "    GenerationConfig,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# PEFT & TRL for RLHF\n",
    "from peft import PeftModel, LoraConfig, TaskType, get_peft_model\n",
    "from trl import (\n",
    "    PPOTrainer,\n",
    "    PPOConfig, \n",
    "    AutoModelForCausalLMWithValueHead,\n",
    "    AutoModelForSeq2SeqLMWithValueHead,\n",
    "    create_reference_model\n",
    ")\n",
    "\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "# Evaluation\n",
    "import evaluate\n",
    "\n",
    "# Utilities\n",
    "from shared_models import HellaSwagEntry\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead79baf",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3400b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../data/hellaswag_format/personal_chat_sessions_train_hellaswag.jsonl\")\n",
    "REWARD_MODEL_PATH = \"../data/models/reward_model_ckpts/checkpoint-3762\"\n",
    "RLHF_CKPTS_DIR = \"../data/models/rlhf_ckpts\"\n",
    "BASE_MODEL_NAME = \"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "172118e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 22,282 examples\n"
     ]
    }
   ],
   "source": [
    "# Function to read JSONL via Pydantic\n",
    "def load_jsonl_pydantic(path: Path):\n",
    "    \"\"\"Yield HellaSwagEntry objects parsed with Pydantic.\"\"\"\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield HellaSwagEntry.model_validate_json(line)\n",
    "\n",
    "# Load records\n",
    "records = list(load_jsonl_pydantic(DATA_PATH))\n",
    "print(f\"Loaded {len(records):,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfb7288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 22,282 examples from ..\\data\\hellaswag_format\\personal_chat_sessions_train_hellaswag.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20053/20053 [00:32<00:00, 623.90 examples/s]\n",
      "Map: 100%|██████████| 2229/2229 [00:02<00:00, 986.72 examples/s] \n"
     ]
    }
   ],
   "source": [
    "def load_jsonl_pydantic(path: Path):\n",
    "    \"\"\"Yield HellaSwagEntry objects parsed with Pydantic.\"\"\"\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield HellaSwagEntry.model_validate_json(line)\n",
    "\n",
    "records = list(load_jsonl_pydantic(DATA_PATH))\n",
    "print(f\"Loaded {len(records):,} examples from {DATA_PATH}\")\n",
    "\n",
    "# Build context-response pairs\n",
    "data_pairs = []\n",
    "for ex in load_jsonl_pydantic(DATA_PATH):\n",
    "    endings    = [ex.ending0, ex.ending1, ex.ending2, ex.ending3, ex.ending4]\n",
    "    human_resp = endings[ex.label].strip()\n",
    "    data_pairs.append({\n",
    "        \"context\": ex.context.strip(),\n",
    "        \"human_resp\": human_resp\n",
    "    })\n",
    "\n",
    "raw_dataset = Dataset.from_list(data_pairs)\n",
    "train_test  = raw_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_ds    = train_test[\"train\"]\n",
    "test_ds     = train_test[\"test\"]\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, device_map=\"auto\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "# Tokenization function\n",
    "max_len = 128\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"context\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_len,\n",
    "    )\n",
    "\n",
    "# Map tokenization over the datasets\n",
    "train_ds = train_ds.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=[\"context\", \"human_resp\"]\n",
    ")\n",
    "test_ds = test_ds.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=[\"context\", \"human_resp\"]\n",
    ")\n",
    "\n",
    "# Use a data collator to batch and pad\n",
    "data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3565d341",
   "metadata": {},
   "source": [
    "#### Load and Prepare Base LLM + LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "946ea7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    print(f\"\\ntrainable model parameters: {trainable_model_params}\\\n",
    "    \\nall model parameters: {all_model_params}\\\n",
    "    \\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d607ecbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "trainable model parameters: 295425    \n",
      "all model parameters: 60802049    \n",
      "percentage of trainable model parameters: 0.49%\n"
     ]
    }
   ],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\",\"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "# Load base model and attach PEFT adapter\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(base_model,lora_config)\n",
    "\n",
    "\n",
    "dppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(\n",
    "    peft_model,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    is_trainable=True\n",
    ")\n",
    "ref_model = create_reference_model(dppo_model)\n",
    "\n",
    "# Load the saved generation_config.json from the base model\n",
    "# TRL throws an error otherwise.\n",
    "gen_config = GenerationConfig.from_pretrained(BASE_MODEL_NAME)\n",
    "gen_config.min_length = 1\n",
    "gen_config.max_new_tokens = 50\n",
    "\n",
    "dppo_model.generation_config = gen_config\n",
    "ref_model.generation_config = gen_config\n",
    "\n",
    "print_number_of_trainable_model_parameters(dppo_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571cdd49",
   "metadata": {},
   "source": [
    "#### Prepare Human-Preference Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38737627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward model labels: {0: 'LABEL_0', 1: 'LABEL_1'}\n"
     ]
    }
   ],
   "source": [
    "reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_PATH, device_map=\"auto\")\n",
    "if reward_tokenizer.pad_token is None:\n",
    "    reward_tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    REWARD_MODEL_PATH,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "value_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    REWARD_MODEL_PATH,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "print(\"Reward model labels:\", reward_model.config.id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cebbbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Inference pipeline (raw logits)\n",
    "preference_pipe = pipeline(\n",
    "    'text-classification',\n",
    "    model=reward_model,\n",
    "    tokenizer=reward_tokenizer,\n",
    "    framework=\"pt\"\n",
    ")\n",
    "reward_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\", \"batch_size\": 16}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e40d629",
   "metadata": {},
   "source": [
    "#### Set up PPO-Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a74d69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    output_dir   = RLHF_CKPTS_DIR,\n",
    "    num_ppo_epochs = 3,\n",
    "    mini_batch_size = 4,\n",
    "    batch_size      = 16\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config,\n",
    "    model        = dppo_model,\n",
    "    ref_model    = ref_model,\n",
    "    processing_class    = tokenizer,\n",
    "    reward_model = reward_model,\n",
    "    value_model  = value_model,\n",
    "    train_dataset = train_ds,\n",
    "    eval_dataset  = test_ds,\n",
    "    data_collator = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13e36713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===training policy===\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (0) must match the size of tensor b (53) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Start RLHF training loop\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mppo_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\source\\Anton\\anton\\.venv\\Lib\\site-packages\\trl\\trainer\\ppo_trainer.py:438\u001b[39m, in \u001b[36mPPOTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    436\u001b[39m response = query_response[:, context_length:]\n\u001b[32m    437\u001b[39m logits = logitss[i : i + args.local_rollout_forward_batch_size]\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m logprob = \u001b[43mselective_log_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m logits\n\u001b[32m    440\u001b[39m torch.cuda.empty_cache()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\source\\Anton\\anton\\.venv\\Lib\\site-packages\\trl\\trainer\\utils.py:1692\u001b[39m, in \u001b[36mselective_log_softmax\u001b[39m\u001b[34m(logits, index)\u001b[39m\n\u001b[32m   1690\u001b[39m     \u001b[38;5;66;03m# loop to reduce peak mem consumption\u001b[39;00m\n\u001b[32m   1691\u001b[39m     logsumexp_values = torch.stack([torch.logsumexp(lg, dim=-\u001b[32m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m lg \u001b[38;5;129;01min\u001b[39;00m logits])\n\u001b[32m-> \u001b[39m\u001b[32m1692\u001b[39m     per_token_logps = \u001b[43mselected_logits\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogsumexp_values\u001b[49m  \u001b[38;5;66;03m# log_softmax(x_i) = x_i - logsumexp(x)\u001b[39;00m\n\u001b[32m   1693\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1694\u001b[39m     \u001b[38;5;66;03m# logsumexp approach is unstable with bfloat16, fall back to slightly less efficent approach\u001b[39;00m\n\u001b[32m   1695\u001b[39m     per_token_logps = []\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (0) must match the size of tensor b (53) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Start RLHF training loop\n",
    "ppo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11946ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_FT_Kernel",
   "language": "python",
   "name": "llm_ft_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

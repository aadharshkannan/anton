{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a7b61b-5f1c-4a03-80ac-fb92022ac8aa",
   "metadata": {},
   "source": [
    "# Evaluate Base vs. CPO‐Trained Model with ROUGE\n",
    "\n",
    "This notebook loads the saved test set, generates outputs from both models, and computes ROUGE-1, ROUGE-2, and ROUGE-L.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Install & Import Dependencies\n",
    "\n",
    "```python\n",
    "# if you haven't already installed these\n",
    "!pip install transformers datasets evaluate accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea711e4-318b-4aa5-9180-0b79404b8b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 18:21:11.015626: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-09 18:21:11.028732: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746814871.046177    5315 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746814871.051527    5315 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-09 18:21:11.068590: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87af1dbb-bc34-4a22-80fe-f52097cbe592",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b0c15-fc99-4bd2-88b4-429b28180c1c",
   "metadata": {},
   "source": [
    "#### Configuration & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a1f3210-7843-47cf-9ec5-e7be15cc8ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust as needed\n",
    "BASE_MODEL_NAME   = \"Qwen/Qwen2.5-0.5B\"\n",
    "CPO_MODEL_PATH    = \"./rlhf_cpo_ckpts/checkpoint-17658\"      # where you saved the fine-tuned model\n",
    "TEST_DS_CSV       = \"./test_cpo_ds.csv\"     # or .pkl if you prefer\n",
    "\n",
    "# Generation hyperparameters\n",
    "MAX_NEW_TOKENS    = 50\n",
    "TEMPERATURE       = 1.0\n",
    "TOP_P             = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14b4585-abaf-4c20-a9a6-a6ef0f0c787e",
   "metadata": {},
   "source": [
    "#### Load the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "948f68cf-db9e-4471-b5aa-f4a910c54c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 630 examples\n"
     ]
    }
   ],
   "source": [
    "# assumes you saved via df_test.to_csv(...)\n",
    "df = pd.read_csv(TEST_DS_CSV)\n",
    "\n",
    "# our “reference” is the human response, strip leading newline if present\n",
    "df[\"reference\"] = df[\"chosen\"].str.lstrip(\"\\n\").str.strip()\n",
    "df = df[[\"prompt\", \"reference\"]].dropna().reset_index(drop=True)\n",
    "print(f\"Loaded {len(df)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff4cc9-f24a-4f30-8524-625723bab155",
   "metadata": {},
   "source": [
    "#### Load Models & Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7da74faf-6606-4b31-b80c-5278974b586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75495b04-02cd-4d59-b76f-b7668d0149f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base model\n",
    "base_tok = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, padding_side=\"left\")\n",
    "if base_tok.pad_token is None:\n",
    "    base_tok.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME)\n",
    "base_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38c4d8e8-db45-4c5c-ac25-4f3cc1fe071a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=896, out_features=896, bias=True)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=896, out_features=4, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=4, out_features=896, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CPO‐trained model\n",
    "cpo_tok = AutoTokenizer.from_pretrained(CPO_MODEL_PATH, padding_side=\"left\")\n",
    "if cpo_tok.pad_token is None:\n",
    "    cpo_tok.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "cpo_model = AutoModelForCausalLM.from_pretrained(CPO_MODEL_PATH)\n",
    "cpo_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe97e85-af7d-4617-9c68-8cad4f09511d",
   "metadata": {},
   "source": [
    "#### Define Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31ab11bf-f50e-4b71-9f2a-17b2ef2e8f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(model, tokenizer, prompts):\n",
    "    \"\"\"Batch-generate continuations for a list of prompts.\"\"\"\n",
    "    responses = []\n",
    "    for prompt in tqdm(prompts, desc=\"Generating\"):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        outs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        # skip the prompt tokens\n",
    "        text = tokenizer.decode(outs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "        responses.append(text.strip())\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53480147-ccc0-40bd-828d-3c30ba3ea176",
   "metadata": {},
   "source": [
    "#### Generate for Both Models one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0caa897e-cff3-4fa8-8d33-7402cbd7e6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8e9fef0f834f8196b01349eeff56d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompts = df[\"prompt\"].tolist()\n",
    "\n",
    "# Base\n",
    "base_preds = generate_responses(base_model, base_tok, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d8632a3-5038-40f4-b5a5-b88669be1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dc9ce36-e0f0-4e85-b8ac-2f20fc7f7114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75b7fb9f07d45d3bf2987ffc533d9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cpo_preds  = generate_responses(cpo_model,  cpo_tok,  prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9211092b-8fd3-4034-a262-82395c3c9fe1",
   "metadata": {},
   "source": [
    "### Compute Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c4957a9-7e93-4dae-a7db-469d975500d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  /opt/conda/bin/python -m pip uninstall [options] <package> ...\n",
      "  /opt/conda/bin/python -m pip uninstall [options] -r <requirements file> ...\n",
      "\n",
      "no such option: -f\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall evaluate -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28ea7e43-83cb-433c-b470-1fca3c9305a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.12/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.12/site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.12/site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk->rouge_score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.12/site-packages (from nltk->rouge_score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from nltk->rouge_score) (4.67.1)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24987 sha256=56eb9478a25fa03f7d0c2dd7941d8b2d677c6e4386b9bf2a66fe6920ba8e9943\n",
      "  Stored in directory: /home/sagemaker-user/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a784e97d-9c57-4061-9085-c0ae07846488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model ROUGE:\n",
      "  rouge1_precision: 0.0092\n",
      "  rouge1_recall: 0.0615\n",
      "  rouge1_fmeasure: 0.0152\n",
      "  rouge2_precision: 0.0012\n",
      "  rouge2_recall: 0.0088\n",
      "  rouge2_fmeasure: 0.0021\n",
      "  rougeL_precision: 0.0091\n",
      "  rougeL_recall: 0.0612\n",
      "  rougeL_fmeasure: 0.0151\n",
      "\n",
      "CPO-Trained Model ROUGE:\n",
      "  rouge1_precision: 0.0161\n",
      "  rouge1_recall: 0.0587\n",
      "  rouge1_fmeasure: 0.0202\n",
      "  rouge2_precision: 0.0010\n",
      "  rouge2_recall: 0.0071\n",
      "  rouge2_fmeasure: 0.0015\n",
      "  rougeL_precision: 0.0161\n",
      "  rougeL_recall: 0.0583\n",
      "  rougeL_fmeasure: 0.0201\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from statistics import mean\n",
    "\n",
    "# Initialize the scorer\n",
    "scorer = rouge_scorer.RougeScorer(\n",
    "    [\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "def compute_rouge_scores(predictions, references):\n",
    "    \"\"\"\n",
    "    Compute average precision, recall, and F1 for each ROUGE type.\n",
    "    Returns a dict like {'rouge1_fmeasure': 0.25, ...}\n",
    "    \"\"\"\n",
    "    # score each example\n",
    "    all_scores = [scorer.score(ref, pred) \n",
    "                  for ref, pred in zip(references, predictions)]\n",
    "    \n",
    "    # aggregate\n",
    "    results = {}\n",
    "    for metric in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "        prec = mean(score[metric].precision for score in all_scores)\n",
    "        rec  = mean(score[metric].recall    for score in all_scores)\n",
    "        f1   = mean(score[metric].fmeasure  for score in all_scores)\n",
    "        results[f\"{metric}_precision\"] = prec\n",
    "        results[f\"{metric}_recall\"]    = rec\n",
    "        results[f\"{metric}_fmeasure\"]  = f1\n",
    "    return results\n",
    "\n",
    "# Compute for base model\n",
    "base_scores = compute_rouge_scores(base_preds, df[\"reference\"].tolist())\n",
    "# Compute for CPO‐trained model\n",
    "cpo_scores  = compute_rouge_scores(cpo_preds,  df[\"reference\"].tolist())\n",
    "\n",
    "# Display\n",
    "print(\"Base Model ROUGE:\")\n",
    "for k, v in base_scores.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nCPO-Trained Model ROUGE:\")\n",
    "for k, v in cpo_scores.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55d47b37",
   "metadata": {},
   "source": [
    "### Reward Model with TRL RewardTrainer\n",
    "This notebook uses the TRL `RewardTrainer` (v0.17.0) to train a reward model on HellaSwag-style chat data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139e5f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import Dataset\n",
    "from shared_models import HellaSwagEntry\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from trl import RewardTrainer, RewardConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d7dbaa",
   "metadata": {},
   "source": [
    "#### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b085a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../data/hellaswag_format/personal_chat_sessions_train_hellaswag.jsonl\")\n",
    "\n",
    "def load_jsonl_pydantic(path):\n",
    "    \"\"\"Yield HellaSwagEntry objects parsed with Pydantic.\"\"\"\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield HellaSwagEntry.model_validate_json(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43653cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pairwise examples\n",
    "pairs = []\n",
    "for ex in load_jsonl_pydantic(DATA_PATH):\n",
    "    endings = [ex.ending0, ex.ending1, ex.ending2, ex.ending3, ex.ending4]\n",
    "    pos_id = ex.label\n",
    "    neg_id = random.choice([i for i in range(5) if i != pos_id])\n",
    "\n",
    "    pos_txt, neg_txt = endings[pos_id].strip(), endings[neg_id].strip()\n",
    "    context = ex.context.strip()\n",
    "\n",
    "    # randomly order A/B\n",
    "    if random.random() < 0.5:\n",
    "        first, second, lbl = pos_txt, neg_txt, 1\n",
    "    else:\n",
    "        first, second, lbl = neg_txt, pos_txt, 0\n",
    "\n",
    "    pairs.append({\n",
    "        \"context\": context,\n",
    "        \"first_resp\": first,\n",
    "        \"second_resp\": second,\n",
    "        \"label\": lbl\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "501a2376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HF Dataset and split\n",
    "dataset = Dataset.from_list(pairs)\n",
    "train_test = dataset.train_test_split(test_size=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362eaffe",
   "metadata": {},
   "source": [
    "#### Prepare for RewardTrainer\n",
    "Convert to the `\"chosen\"` / `\"rejected\"` format required by RewardTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f3f813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77643b74e51f42878dff67cfa43b0175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20053 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec610aef4ff94fc4a7c6bb20df83a888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def map_to_reward(examples):\n",
    "    chosen, rejected = [], []\n",
    "    for lbl, a, b in zip(examples[\"label\"], examples[\"first_resp\"], examples[\"second_resp\"]):\n",
    "        if lbl == 1:\n",
    "            chosen.append(a)\n",
    "            rejected.append(b)\n",
    "        else:\n",
    "            chosen.append(b)\n",
    "            rejected.append(a)\n",
    "    return {\"chosen\": chosen, \"rejected\": rejected}\n",
    "\n",
    "rm_dataset = train_test.map(\n",
    "    map_to_reward,\n",
    "    batched=True,\n",
    "    remove_columns=train_test[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caadde19",
   "metadata": {},
   "source": [
    "#### Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c4a80d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"trl-internal-testing/tiny-Qwen2ForSequenceClassification-2.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "tokenizer.max_length = 128\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "tokenizer.chat_template = getattr(self.tokenizer, \"chat_template\", None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75ef797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single‐scalar head for reward\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    num_labels=1,\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5cfc2f",
   "metadata": {},
   "source": [
    "#### LoRA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0180b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604c51eb",
   "metadata": {},
   "source": [
    " #### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ec35f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = RewardConfig(\n",
    "    output_dir=\"../data/models/reward_model_ckpts_test\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    max_length=128,\n",
    "    disable_dropout=False,  # keep dropout active during training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40309fdc",
   "metadata": {},
   "source": [
    "#### Initialize & Run RewardTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79b3d3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22438cdb86a34f109b2a142b4138132f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20053 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d462a6dbd4204f3e971239dbaad67913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20053 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc21cf99e87a4e56966f48585189ee87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/20053 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be797d386d43428080e342b35118455a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e8bfdde4504ebf822b4f43ca3bd1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c4a77fd6c14a30bc561835bea8d876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=rm_dataset[\"train\"],\n",
    "    eval_dataset=rm_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71806343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> chosen_text </span>┃<span style=\"font-weight: bold\"> rejected_text                            </span>┃<span style=\"font-weight: bold\"> logits           </span>┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ 60k         │ Wow, impressive amount!                  │ [0.474, 0.526]   │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Saaptiya    │ I completely get that, it's frustrating. │ [0.5144, 0.4856] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Sure        │ Sounds good!                             │ [0.5029, 0.4971] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Thanks      │ Awesome, that's a healthy amount!        │ [0.5019, 0.4981] │\n",
       "└─────────────┴──────────────────────────────────────────┴──────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mchosen_text\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrejected_text                           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlogits          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ 60k         │ Wow, impressive amount!                  │ [0.474, 0.526]   │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Saaptiya    │ I completely get that, it's frustrating. │ [0.5144, 0.4856] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Sure        │ Sounds good!                             │ [0.5029, 0.4971] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Thanks      │ Awesome, that's a healthy amount!        │ [0.5019, 0.4981] │\n",
       "└─────────────┴──────────────────────────────────────────┴──────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 01:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: {'eval_loss': 0.6993284225463867, 'eval_model_preparation_time': 0.0012, 'eval_accuracy': 0.48742138364779874, 'eval_runtime': 1.863, 'eval_samples_per_second': 1194.852, 'eval_steps_per_second': 37.574}\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline:\", trainer.evaluate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8c0cb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3753' max='3753' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3753/3753 03:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.676200</td>\n",
       "      <td>0.660428</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.730458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.640300</td>\n",
       "      <td>0.634826</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.810872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.628500</td>\n",
       "      <td>0.626181</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.822102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> chosen_text </span>┃<span style=\"font-weight: bold\"> rejected_text                            </span>┃<span style=\"font-weight: bold\"> logits           </span>┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ 60k         │ Wow, impressive amount!                  │ [0.4975, 0.5025] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Saaptiya    │ I completely get that, it's frustrating. │ [0.568, 0.432]   │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Sure        │ Sounds good!                             │ [0.5545, 0.4455] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Thanks      │ Awesome, that's a healthy amount!        │ [0.4782, 0.5218] │\n",
       "└─────────────┴──────────────────────────────────────────┴──────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mchosen_text\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrejected_text                           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlogits          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ 60k         │ Wow, impressive amount!                  │ [0.4975, 0.5025] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Saaptiya    │ I completely get that, it's frustrating. │ [0.568, 0.432]   │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Sure        │ Sounds good!                             │ [0.5545, 0.4455] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Thanks      │ Awesome, that's a healthy amount!        │ [0.4782, 0.5218] │\n",
       "└─────────────┴──────────────────────────────────────────┴──────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> chosen_text </span>┃<span style=\"font-weight: bold\"> rejected_text                            </span>┃<span style=\"font-weight: bold\"> logits           </span>┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ 60k         │ Wow, impressive amount!                  │ [0.5031, 0.4969] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Saaptiya    │ I completely get that, it's frustrating. │ [0.6089, 0.3911] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Sure        │ Sounds good!                             │ [0.5761, 0.4239] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Thanks      │ Awesome, that's a healthy amount!        │ [0.4796, 0.5204] │\n",
       "└─────────────┴──────────────────────────────────────────┴──────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mchosen_text\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrejected_text                           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlogits          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ 60k         │ Wow, impressive amount!                  │ [0.5031, 0.4969] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Saaptiya    │ I completely get that, it's frustrating. │ [0.6089, 0.3911] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Sure        │ Sounds good!                             │ [0.5761, 0.4239] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Thanks      │ Awesome, that's a healthy amount!        │ [0.4796, 0.5204] │\n",
       "└─────────────┴──────────────────────────────────────────┴──────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> chosen_text </span>┃<span style=\"font-weight: bold\"> rejected_text                            </span>┃<span style=\"font-weight: bold\"> logits           </span>┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ 60k         │ Wow, impressive amount!                  │ [0.5103, 0.4897] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Saaptiya    │ I completely get that, it's frustrating. │ [0.6192, 0.3808] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Sure        │ Sounds good!                             │ [0.5903, 0.4097] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Thanks      │ Awesome, that's a healthy amount!        │ [0.4794, 0.5206] │\n",
       "└─────────────┴──────────────────────────────────────────┴──────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mchosen_text\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrejected_text                           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlogits          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ 60k         │ Wow, impressive amount!                  │ [0.5103, 0.4897] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Saaptiya    │ I completely get that, it's frustrating. │ [0.6192, 0.3808] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Sure        │ Sounds good!                             │ [0.5903, 0.4097] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Thanks      │ Awesome, that's a healthy amount!        │ [0.4794, 0.5206] │\n",
       "└─────────────┴──────────────────────────────────────────┴──────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3753, training_loss=0.6526059222608892, metrics={'train_runtime': 200.7758, 'train_samples_per_second': 298.99, 'train_steps_per_second': 18.692, 'total_flos': 0.0, 'train_loss': 0.6526059222608892, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8da37289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> chosen_text </span>┃<span style=\"font-weight: bold\"> rejected_text                            </span>┃<span style=\"font-weight: bold\"> logits           </span>┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ 60k         │ Wow, impressive amount!                  │ [0.517, 0.483]   │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Saaptiya    │ I completely get that, it's frustrating. │ [0.623, 0.377]   │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Sure        │ Sounds good!                             │ [0.5913, 0.4087] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Thanks      │ Awesome, that's a healthy amount!        │ [0.48, 0.52]     │\n",
       "└─────────────┴──────────────────────────────────────────┴──────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mchosen_text\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrejected_text                           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlogits          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ 60k         │ Wow, impressive amount!                  │ [0.517, 0.483]   │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Saaptiya    │ I completely get that, it's frustrating. │ [0.623, 0.377]   │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Sure        │ Sounds good!                             │ [0.5913, 0.4087] │\n",
       "├─────────────┼──────────────────────────────────────────┼──────────────────┤\n",
       "│ Thanks      │ Awesome, that's a healthy amount!        │ [0.48, 0.52]     │\n",
       "└─────────────┴──────────────────────────────────────────┴──────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final: {'eval_loss': 0.6261805891990662, 'eval_model_preparation_time': 0.0012, 'eval_accuracy': 0.8221024258760108, 'eval_runtime': 1.751, 'eval_samples_per_second': 1271.254, 'eval_steps_per_second': 39.977, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Final:\", trainer.evaluate())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_FT_Kernel",
   "language": "python",
   "name": "llm_ft_kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

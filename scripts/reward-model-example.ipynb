{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69b5c6bb",
   "metadata": {},
   "source": [
    "### Reward Model\n",
    "This notebook constructs the Reward Model that will be used in a PPO step at a later stage. Still takes the dilbert uncased and builds on top of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86b5f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from shared_models import HellaSwagEntry\n",
    "from transformers import (AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification,\n",
    "                          Trainer, TrainingArguments)\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f98f2e9",
   "metadata": {},
   "source": [
    "#### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ee1d906",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../data/hellaswag_format/personal_chat_sessions_train_hellaswag.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83aa6ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context', 'first_resp', 'second_resp', 'label'],\n",
       "    num_rows: 22282\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_jsonl_pydantic(path):\n",
    "    \"\"\"Yield HellaSwagEntry objects parsed with Pydantic.\"\"\"\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield HellaSwagEntry.model_validate_json(line)\n",
    "\n",
    "# Build pairwise examples\n",
    "pairs = []\n",
    "for ex in load_jsonl_pydantic(DATA_PATH):\n",
    "    endings = [ex.ending0, ex.ending1, ex.ending2, ex.ending3, ex.ending4]\n",
    "    pos_id = ex.label\n",
    "    neg_id = random.choice([i for i in range(5) if i != pos_id])\n",
    "\n",
    "    pos_txt, neg_txt = endings[pos_id].strip(), endings[neg_id].strip()\n",
    "    context = ex.context.strip()\n",
    "\n",
    "    # randomly order A/B\n",
    "    if random.random() < 0.5:\n",
    "        first, second, lbl = pos_txt, neg_txt, 1\n",
    "    else:\n",
    "        first, second, lbl = neg_txt, pos_txt, 0\n",
    "\n",
    "    pairs.append({\n",
    "        \"context\": context,\n",
    "        \"first_resp\": first,\n",
    "        \"second_resp\": second,\n",
    "        \"label\": lbl\n",
    "    })\n",
    "\n",
    "# Create HF Dataset\n",
    "dataset = Dataset.from_list(pairs)\n",
    "train_test = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1190b89",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac6645df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac359329b014bf9ac2a65f210ed157c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aadhu\\source\\Anton\\anton\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\aadhu\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b186056599574e98a29878d357b8e06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed185db46884d10bf773b4804d19a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92a220389ba4a49bfa728bde5c9fc6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7395ea1b1dc740adbd67d7f5e1dedefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20053 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1273640d5cb4cddabd05dec4586d9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_ckpt = \"google-bert/bert-base-uncased\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "def tokenize_fn(examples):\n",
    "    # Hugging Face will do: [CLS] context [SEP] first_resp [SEP] second_resp [SEP]\n",
    "    tokenizerSnd= tokenizer(\n",
    "        examples[\"context\"],\n",
    "        [f\"{a} {tokenizer.sep_token} {b}\" for a, b in zip(examples[\"first_resp\"], examples[\"second_resp\"])],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    return tokenizerSnd\n",
    "\n",
    "tokenized = train_test.map(tokenize_fn,\n",
    "                           batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb871d18",
   "metadata": {},
   "source": [
    "#### Model LoRA Setup for Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50f155eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    print(f\"\\ntrainable model parameters: {trainable_model_params}\\\n",
    "    \\nall model parameters: {all_model_params}\\\n",
    "    \\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8277a6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n",
      "bert\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "bert.embeddings\n",
      "BertEmbeddings(\n",
      "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 768)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.embeddings.word_embeddings\n",
      "Embedding(30522, 768, padding_idx=0)\n",
      "bert.embeddings.position_embeddings\n",
      "Embedding(512, 768)\n",
      "bert.embeddings.token_type_embeddings\n",
      "Embedding(2, 768)\n",
      "bert.embeddings.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.embeddings.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder\n",
      "BertEncoder(\n",
      "  (layer): ModuleList(\n",
      "    (0-11): 12 x BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSdpaSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer\n",
      "ModuleList(\n",
      "  (0-11): 12 x BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSdpaSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (intermediate_act_fn): GELUActivation()\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.0\n",
      "BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.0.attention\n",
      "BertAttention(\n",
      "  (self): BertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.0.attention.self\n",
      "BertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.0.attention.self.query\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.0.attention.self.key\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.0.attention.self.value\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.0.attention.self.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.0.attention.output\n",
      "BertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.0.attention.output.dense\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.0.attention.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.0.intermediate\n",
      "BertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "bert.encoder.layer.0.intermediate.dense\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "bert.encoder.layer.0.intermediate.intermediate_act_fn\n",
      "GELUActivation()\n",
      "bert.encoder.layer.0.output\n",
      "BertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.0.output.dense\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "bert.encoder.layer.0.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.0.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.1\n",
      "BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.1.attention\n",
      "BertAttention(\n",
      "  (self): BertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.1.attention.self\n",
      "BertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.1.attention.self.query\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.1.attention.self.key\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.1.attention.self.value\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.1.attention.self.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.1.attention.output\n",
      "BertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.1.attention.output.dense\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.1.attention.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.1.attention.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.1.intermediate\n",
      "BertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "bert.encoder.layer.1.intermediate.dense\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "bert.encoder.layer.1.intermediate.intermediate_act_fn\n",
      "GELUActivation()\n",
      "bert.encoder.layer.1.output\n",
      "BertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.1.output.dense\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "bert.encoder.layer.1.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.1.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.2\n",
      "BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.2.attention\n",
      "BertAttention(\n",
      "  (self): BertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.2.attention.self\n",
      "BertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.2.attention.self.query\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.2.attention.self.key\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.2.attention.self.value\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.2.attention.self.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.2.attention.output\n",
      "BertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.2.attention.output.dense\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.2.attention.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.2.attention.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.2.intermediate\n",
      "BertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "bert.encoder.layer.2.intermediate.dense\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "bert.encoder.layer.2.intermediate.intermediate_act_fn\n",
      "GELUActivation()\n",
      "bert.encoder.layer.2.output\n",
      "BertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.2.output.dense\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "bert.encoder.layer.2.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.2.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.3\n",
      "BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.3.attention\n",
      "BertAttention(\n",
      "  (self): BertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.3.attention.self\n",
      "BertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.3.attention.self.query\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.3.attention.self.key\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.3.attention.self.value\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.3.attention.self.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.3.attention.output\n",
      "BertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.3.attention.output.dense\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.3.attention.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.3.attention.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.3.intermediate\n",
      "BertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "bert.encoder.layer.3.intermediate.dense\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "bert.encoder.layer.3.intermediate.intermediate_act_fn\n",
      "GELUActivation()\n",
      "bert.encoder.layer.3.output\n",
      "BertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.3.output.dense\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "bert.encoder.layer.3.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.3.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.4\n",
      "BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.4.attention\n",
      "BertAttention(\n",
      "  (self): BertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.4.attention.self\n",
      "BertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.4.attention.self.query\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.4.attention.self.key\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.4.attention.self.value\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.4.attention.self.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.4.attention.output\n",
      "BertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.4.attention.output.dense\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.4.attention.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.4.attention.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.4.intermediate\n",
      "BertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "bert.encoder.layer.4.intermediate.dense\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "bert.encoder.layer.4.intermediate.intermediate_act_fn\n",
      "GELUActivation()\n",
      "bert.encoder.layer.4.output\n",
      "BertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.4.output.dense\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "bert.encoder.layer.4.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.4.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.5\n",
      "BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.5.attention\n",
      "BertAttention(\n",
      "  (self): BertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.5.attention.self\n",
      "BertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.5.attention.self.query\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.5.attention.self.key\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.5.attention.self.value\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.5.attention.self.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.5.attention.output\n",
      "BertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.5.attention.output.dense\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.5.attention.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.5.attention.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.5.intermediate\n",
      "BertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "bert.encoder.layer.5.intermediate.dense\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "bert.encoder.layer.5.intermediate.intermediate_act_fn\n",
      "GELUActivation()\n",
      "bert.encoder.layer.5.output\n",
      "BertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.5.output.dense\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "bert.encoder.layer.5.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.5.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.6\n",
      "BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.6.attention\n",
      "BertAttention(\n",
      "  (self): BertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.6.attention.self\n",
      "BertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.6.attention.self.query\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.6.attention.self.key\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.6.attention.self.value\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.6.attention.self.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.6.attention.output\n",
      "BertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.6.attention.output.dense\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.6.attention.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.6.attention.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.6.intermediate\n",
      "BertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "bert.encoder.layer.6.intermediate.dense\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "bert.encoder.layer.6.intermediate.intermediate_act_fn\n",
      "GELUActivation()\n",
      "bert.encoder.layer.6.output\n",
      "BertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.6.output.dense\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "bert.encoder.layer.6.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.6.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.7\n",
      "BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.7.attention\n",
      "BertAttention(\n",
      "  (self): BertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.7.attention.self\n",
      "BertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.7.attention.self.query\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.7.attention.self.key\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.7.attention.self.value\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.7.attention.self.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.7.attention.output\n",
      "BertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.7.attention.output.dense\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.7.attention.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.7.attention.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.7.intermediate\n",
      "BertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "bert.encoder.layer.7.intermediate.dense\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "bert.encoder.layer.7.intermediate.intermediate_act_fn\n",
      "GELUActivation()\n",
      "bert.encoder.layer.7.output\n",
      "BertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.7.output.dense\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "bert.encoder.layer.7.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.7.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.8\n",
      "BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.8.attention\n",
      "BertAttention(\n",
      "  (self): BertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.8.attention.self\n",
      "BertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.8.attention.self.query\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.8.attention.self.key\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.8.attention.self.value\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.8.attention.self.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.8.attention.output\n",
      "BertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.8.attention.output.dense\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.8.attention.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.8.attention.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.8.intermediate\n",
      "BertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "bert.encoder.layer.8.intermediate.dense\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "bert.encoder.layer.8.intermediate.intermediate_act_fn\n",
      "GELUActivation()\n",
      "bert.encoder.layer.8.output\n",
      "BertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.8.output.dense\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "bert.encoder.layer.8.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.8.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.9\n",
      "BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.9.attention\n",
      "BertAttention(\n",
      "  (self): BertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.9.attention.self\n",
      "BertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.9.attention.self.query\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.9.attention.self.key\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.9.attention.self.value\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.9.attention.self.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.9.attention.output\n",
      "BertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.9.attention.output.dense\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.9.attention.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.9.attention.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.9.intermediate\n",
      "BertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "bert.encoder.layer.9.intermediate.dense\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "bert.encoder.layer.9.intermediate.intermediate_act_fn\n",
      "GELUActivation()\n",
      "bert.encoder.layer.9.output\n",
      "BertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.9.output.dense\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "bert.encoder.layer.9.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.9.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.10\n",
      "BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.10.attention\n",
      "BertAttention(\n",
      "  (self): BertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.10.attention.self\n",
      "BertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.10.attention.self.query\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.10.attention.self.key\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.10.attention.self.value\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.10.attention.self.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.10.attention.output\n",
      "BertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.10.attention.output.dense\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.10.attention.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.10.attention.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.10.intermediate\n",
      "BertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "bert.encoder.layer.10.intermediate.dense\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "bert.encoder.layer.10.intermediate.intermediate_act_fn\n",
      "GELUActivation()\n",
      "bert.encoder.layer.10.output\n",
      "BertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.10.output.dense\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "bert.encoder.layer.10.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.10.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.11\n",
      "BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.11.attention\n",
      "BertAttention(\n",
      "  (self): BertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "bert.encoder.layer.11.attention.self\n",
      "BertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.11.attention.self.query\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.11.attention.self.key\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.11.attention.self.value\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.11.attention.self.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.11.attention.output\n",
      "BertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.11.attention.output.dense\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.encoder.layer.11.attention.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.11.attention.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.encoder.layer.11.intermediate\n",
      "BertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "bert.encoder.layer.11.intermediate.dense\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "bert.encoder.layer.11.intermediate.intermediate_act_fn\n",
      "GELUActivation()\n",
      "bert.encoder.layer.11.output\n",
      "BertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "bert.encoder.layer.11.output.dense\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "bert.encoder.layer.11.output.LayerNorm\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "bert.encoder.layer.11.output.dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "bert.pooler\n",
      "BertPooler(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      ")\n",
      "bert.pooler.dense\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "bert.pooler.activation\n",
      "Tanh()\n",
      "dropout\n",
      "Dropout(p=0.1, inplace=False)\n",
      "classifier\n",
      "Linear(in_features=768, out_features=2, bias=True)\n",
      "\n",
      "trainable model parameters: 148994    \n",
      "all model parameters: 109632772    \n",
      "percentage of trainable model parameters: 0.14%\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "#Resize embeddings to match new tokenizer vocab\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Models config should know about the pad token\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "for name, module in base_model.named_modules():\n",
    "    print(name)\n",
    "    print(module)\n",
    "\n",
    "# Attach LoRA for parameter-efficient fine-tuning\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    inference_mode=False,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "\n",
    "print_number_of_trainable_model_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4457245",
   "metadata": {},
   "source": [
    "#### Training with Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa9d4b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aadhu\\AppData\\Local\\Temp\\ipykernel_14732\\2318968774.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(axis=-1)\n",
    "    return accuracy.compute(predictions=preds, references=p.label_ids)\n",
    "\n",
    "# TrainingArguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../data/models/reward_model_ckpts\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a51ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 55:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: {'eval_loss': 0.7118020057678223, 'eval_model_preparation_time': 0.0054, 'eval_accuracy': 0.4791386271870794, 'eval_runtime': 132.0535, 'eval_samples_per_second': 16.88, 'eval_steps_per_second': 0.53}\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline:\", trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45cadefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3762' max='3762' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3762/3762 2:04:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.695500</td>\n",
       "      <td>0.689722</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.534769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.402200</td>\n",
       "      <td>0.344743</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.856438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.359300</td>\n",
       "      <td>0.307014</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.873934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3762, training_loss=0.5303614736553964, metrics={'train_runtime': 7497.749, 'train_samples_per_second': 8.024, 'train_steps_per_second': 0.502, 'total_flos': 3964008332325888.0, 'train_loss': 0.5303614736553964, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36527dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 01:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final: {'eval_loss': 0.3070140779018402, 'eval_model_preparation_time': 0.0054, 'eval_accuracy': 0.8739344997756842, 'eval_runtime': 121.3309, 'eval_samples_per_second': 18.371, 'eval_steps_per_second': 0.577, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Final:\", trainer.evaluate())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_FT_Kernel",
   "language": "python",
   "name": "llm_ft_kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b57339e",
   "metadata": {},
   "source": [
    "# RLHF with PPOTrainer and PEFT (LoRA)\n",
    "This notebook demonstrates how to fine-tune a base language model using Proximal Policy Optimization (PPO) with Parameter-Efficient Fine-Tuning (PEFT) via LoRA adapters. The PPOTrainer will instantiate a reference model internally (i.e., `ref_model=None`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54948b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad775bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Hugging Face Transformers & Datasets\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "# PEFT for Parameter-Efficient Fine-Tuning\n",
    "from peft import PeftModel, LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# TRL for RLHF\n",
    "from trl import (\n",
    "    PPOTrainer,\n",
    "    PPOConfig,\n",
    "    AutoModelForCausalLMWithValueHead\n",
    ")\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "# Evaluation\n",
    "import evaluate\n",
    "\n",
    "# Utilities\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6497787a",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7d92889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fdd6d4097a49c797b2cc31b5c53563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/22282 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_PATH = Path(\"../data/hellaswag_format/personal_chat_sessions_train_hellaswag.jsonl\")\n",
    "MIN_WORDS = 3\n",
    "\n",
    "def load_jsonl_pydantic(path: Path):\n",
    "    from shared_models import HellaSwagEntry\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield HellaSwagEntry.model_validate_json(line)\n",
    "\n",
    "# Starting from your raw `Dataset`\n",
    "def has_enough_words(example):\n",
    "    return len(example[\"context\"].split()) >= MIN_WORDS\n",
    "\n",
    "data_pairs = []\n",
    "for ex in load_jsonl_pydantic(DATA_PATH):\n",
    "    endings = [ex.ending0, ex.ending1, ex.ending2, ex.ending3, ex.ending4]\n",
    "    human_resp = endings[ex.label].strip()\n",
    "    data_pairs.append({\n",
    "        \"context\": ex.context.strip(),\n",
    "        \"human_resp\": human_resp\n",
    "    })\n",
    "\n",
    "raw_dataset = Dataset.from_list(data_pairs)\n",
    "raw_dataset = raw_dataset.filter(has_enough_words)\n",
    "\n",
    "train_test = raw_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_ds, test_ds = train_test[\"train\"], train_test[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648598a0",
   "metadata": {},
   "source": [
    "## 2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37abe1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f850f9c83f8241efb949517a5ec2e583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19955 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6ccd4b002a4e268feca2f33afa92b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2218 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_MODEL_NAME = \"trl-internal-testing/tiny-Qwen2ForCausalLM-2.5\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "tokenizer.max_length =55\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"context\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=55\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=train_ds.column_names)\n",
    "test_ds  = test_ds.map(tokenize_fn, batched=True, remove_columns=test_ds.column_names)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e5333d",
   "metadata": {},
   "source": [
    "## 3. Configure PEFT (LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3233c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7d40eb",
   "metadata": {},
   "source": [
    "## 4. Load Base Model with Value Head and Attach LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f372e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A <class 'transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM'> model is loaded from 'trl-internal-testing/tiny-Qwen2ForCausalLM-2.5', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token_id: 151643\n",
      "vocab size: 151665\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    is_trainable=True\n",
    ")\n",
    "\n",
    "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
    "print(\"vocab size:\", len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1bb093",
   "metadata": {},
   "source": [
    "## 5. Generation Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "594e79f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_config = GenerationConfig.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "model.generation_config = gen_config\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba6d8a8",
   "metadata": {},
   "source": [
    "## 6. Reward Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40909951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "C:\\Users\\aadhu\\source\\Anton\\anton\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "REWARD_MODEL_PATH = \"../data/models/reward_model_ckpts_test/checkpoint-3753\"\n",
    "\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_PATH)\n",
    "reward_tokenizer.max_length = 128\n",
    "\n",
    "if reward_tokenizer.pad_token is None:\n",
    "    reward_tokenizer.pad_token = reward_tokenizer.eos_token\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    REWARD_MODEL_PATH,\n",
    "    num_labels=1\n",
    ")\n",
    "\n",
    "value_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    REWARD_MODEL_PATH,\n",
    "    num_labels=1\n",
    ")\n",
    "\n",
    "value_model.config.pad_token_id = reward_tokenizer.pad_token_id\n",
    "reward_model.config.pad_token_id = reward_tokenizer.pad_token_id\n",
    "\n",
    "preference_pipe = pipeline(\n",
    "    'text-classification',\n",
    "    model=reward_model,\n",
    "    tokenizer=reward_tokenizer,\n",
    "    framework=\"pt\",\n",
    "    return_all_scores=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8730743e",
   "metadata": {},
   "source": [
    "## 7. PPO Trainer Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fff6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import traceback\n",
    "from transformers import GenerationConfig\n",
    "import trl.trainer.utils as utils  # adjust if your utils path is different\n",
    "\n",
    "# Keep a reference if you ever want to restore the original\n",
    "_orig_generate = utils.generate\n",
    "\n",
    "def debug_generate(\n",
    "    lm_backbone: torch.nn.Module,\n",
    "    queries: torch.Tensor,\n",
    "    pad_token_id: int,\n",
    "    generation_config: GenerationConfig,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    context_length = queries.shape[1]\n",
    "    attention_mask = queries != pad_token_id\n",
    "    input_ids = torch.masked_fill(queries, ~attention_mask, 0)\n",
    "\n",
    "     # ── NEW: skip any example with zero non-pad tokens ────────────────────────────\n",
    "    # If *every* token in *all* queries is pad, we bail out in one go:\n",
    "    if attention_mask.sum().item() == 0:\n",
    "        # Build a “no-context” output: echo the queries + a single EOS per example\n",
    "        batch_size = queries.size(0)\n",
    "        # sequences: [original_queries | eos_token]\n",
    "        eos_seq = queries.new_full((batch_size, 1), lm_backbone.config.eos_token_id)\n",
    "        seqs = torch.cat((queries, eos_seq), dim=1)\n",
    "        # logits: zeros of shape [batch_size, max_new_tokens, vocab_size]\n",
    "        max_new = generation_config.max_new_tokens\n",
    "        vocab   = lm_backbone.config.vocab_size\n",
    "        logits  = queries.new_zeros((batch_size, max_new, vocab))\n",
    "        return seqs, logits\n",
    "    # ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "    # ─── Pre‐generate diagnostics ───────────────────────────────────────────────\n",
    "    print(f\"[debug_generate] queries.shape       = {queries.shape}   device={queries.device}\")\n",
    "    print(f\"[debug_generate] pad_token_id        = {pad_token_id}\")\n",
    "    print(f\"[debug_generate] context_length      = {context_length}\")\n",
    "    print(f\"[debug_generate] max(input_ids)      = {input_ids.max().item()}   vocab_size={lm_backbone.config.vocab_size}\")\n",
    "    print(f\"[debug_generate] attention_mask.sum = {attention_mask.sum().item()} tokens unmasked\")\n",
    "    print(f\"[debug_generate] generation_config   = {generation_config}\")\n",
    "\n",
    "    # ─── Call into the model and catch CUDA asserts exactly here ───────────────\n",
    "    try:\n",
    "        torch.cuda.synchronize()\n",
    "        output = lm_backbone.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "        torch.cuda.synchronize()\n",
    "    except Exception:\n",
    "        print(\"✖ Error inside lm_backbone.generate()\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "    # ─── Post‐generate diagnostics ──────────────────────────────────────────────\n",
    "    print(f\"[debug_generate] output.sequences.shape = {output.sequences.shape}\")\n",
    "    print(f\"[debug_generate] # of output.scores     = {len(output.scores)}\")\n",
    "    if len(output.scores) > 0:\n",
    "        print(f\"[debug_generate] output.scores[0].shape = {output.scores[0].shape}\")\n",
    "\n",
    "    # ─── Stack and concat ──────────────────────────────────────────────────────\n",
    "    try:\n",
    "        logits = torch.stack(output.scores, dim=1)\n",
    "    except Exception:\n",
    "        print(\"✖ Error stacking logits\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "    seqs = torch.cat((queries, output.sequences[:, context_length:]), dim=1)\n",
    "    print(f\"[debug_generate] final seqs.shape        = {seqs.shape}\")\n",
    "    print(f\"[debug_generate] final logits.shape      = {logits.shape}\")\n",
    "\n",
    "    return seqs, logits\n",
    "\n",
    "# Monkey-patch the TRL utils.generate to our debug version\n",
    "utils.generate = debug_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "572e0b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = PPOConfig(\n",
    "    output_dir   = \"../data/models/rlhf_ckpts\",\n",
    "    num_ppo_epochs = 3,\n",
    "    batch_size      = 16,\n",
    "    mini_batch_size = 4\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    ppo_config,\n",
    "    model        = model,\n",
    "    ref_model    = None,\n",
    "    peft_config = lora_config,\n",
    "    processing_class    = tokenizer,\n",
    "    reward_model = reward_model,\n",
    "    value_model = value_model,\n",
    "    train_dataset= train_ds,\n",
    "    eval_dataset = test_ds,\n",
    "    data_collator= data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389e7c63",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8377eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===training policy===\n",
      "[debug_generate] queries.shape       = torch.Size([8, 55])   device=cuda:0\n",
      "[debug_generate] pad_token_id        = 151643\n",
      "[debug_generate] context_length      = 55\n",
      "[debug_generate] max(input_ids)      = 90159   vocab_size=151665\n",
      "[debug_generate] attention_mask.sum = 351 tokens unmasked\n",
      "[debug_generate] generation_config   = GenerationConfig {\n",
      "  \"do_sample\": true,\n",
      "  \"max_new_tokens\": 53,\n",
      "  \"temperature\": 0.7000000999999999,\n",
      "  \"top_k\": 0.0\n",
      "}\n",
      "\n",
      "[debug_generate] output.sequences.shape = torch.Size([8, 108])\n",
      "[debug_generate] # of output.scores     = 53\n",
      "[debug_generate] output.scores[0].shape = torch.Size([8, 151665])\n",
      "[debug_generate] final seqs.shape        = torch.Size([8, 108])\n",
      "[debug_generate] final logits.shape      = torch.Size([8, 53, 151665])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='7484' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/7484 : < :, Epoch 0.00/3.0]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug_generate] queries.shape       = torch.Size([8, 55])   device=cuda:0\n",
      "[debug_generate] pad_token_id        = 151643\n",
      "[debug_generate] context_length      = 55\n",
      "[debug_generate] max(input_ids)      = 144736   vocab_size=151665\n",
      "[debug_generate] attention_mask.sum = 399 tokens unmasked\n",
      "[debug_generate] generation_config   = GenerationConfig {\n",
      "  \"do_sample\": true,\n",
      "  \"max_new_tokens\": 53,\n",
      "  \"temperature\": 0.0100001,\n",
      "  \"top_k\": 0.0\n",
      "}\n",
      "\n",
      "✖ Error inside lm_backbone.generate()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aadhu\\AppData\\Local\\Temp\\ipykernel_1692\\729358063.py\", line 45, in debug_generate\n",
      "    output = lm_backbone.generate(\n",
      "        input_ids=input_ids,\n",
      "    ...<3 lines>...\n",
      "        output_scores=True,\n",
      "    )\n",
      "  File \"C:\\Users\\aadhu\\source\\Anton\\anton\\.venv\\Lib\\site-packages\\peft\\peft_model.py\", line 1875, in generate\n",
      "    outputs = self.base_model.generate(*args, **kwargs)\n",
      "  File \"C:\\Users\\aadhu\\source\\Anton\\anton\\.venv\\Lib\\site-packages\\trl\\models\\modeling_value_head.py\", line 199, in generate\n",
      "    return self.pretrained_model.generate(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadhu\\source\\Anton\\anton\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\aadhu\\source\\Anton\\anton\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py\", line 2465, in generate\n",
      "    result = self._sample(\n",
      "        input_ids,\n",
      "    ...<5 lines>...\n",
      "        **model_kwargs,\n",
      "    )\n",
      "  File \"C:\\Users\\aadhu\\source\\Anton\\anton\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py\", line 3476, in _sample\n",
      "    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
      "                  ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mppo_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\source\\Anton\\anton\\.venv\\Lib\\site-packages\\trl\\trainer\\ppo_trainer.py:669\u001b[39m, in \u001b[36mPPOTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    666\u001b[39m gc.collect()\n\u001b[32m    668\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.num_sample_generations > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (update - \u001b[32m1\u001b[39m) % \u001b[38;5;28mself\u001b[39m.sample_generations_freq == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_completions\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    670\u001b[39m     torch.cuda.empty_cache()\n\u001b[32m    671\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m (\n\u001b[32m    672\u001b[39m     query_responses,\n\u001b[32m    673\u001b[39m     responses,\n\u001b[32m   (...)\u001b[39m\u001b[32m    688\u001b[39m     returns,\n\u001b[32m    689\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\source\\Anton\\anton\\.venv\\Lib\\site-packages\\trl\\trainer\\ppo_trainer.py:717\u001b[39m, in \u001b[36mPPOTrainer.generate_completions\u001b[39m\u001b[34m(self, sampling)\u001b[39m\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    716\u001b[39m     context_length = query.shape[\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m717\u001b[39m     query_response, _ = \u001b[43mbatch_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m        \u001b[49m\u001b[43munwrapped_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    724\u001b[39m     response = query_response[:, context_length:]\n\u001b[32m    725\u001b[39m     postprocessed_response = response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\source\\Anton\\anton\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\source\\Anton\\anton\\.venv\\Lib\\site-packages\\trl\\trainer\\utils.py:1379\u001b[39m, in \u001b[36mbatch_generation\u001b[39m\u001b[34m(model, queries, local_rollout_forward_batch_size, pad_token_id, generation_config)\u001b[39m\n\u001b[32m   1377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, batch_size, local_rollout_forward_batch_size):\n\u001b[32m   1378\u001b[39m     query = queries[i : i + local_rollout_forward_batch_size]\n\u001b[32m-> \u001b[39m\u001b[32m1379\u001b[39m     query_response, logits = \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1385\u001b[39m     query_responses.append(query_response)\n\u001b[32m   1386\u001b[39m     logitss.append(logits)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mdebug_generate\u001b[39m\u001b[34m(lm_backbone, queries, pad_token_id, generation_config)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     44\u001b[39m     torch.cuda.synchronize()\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     output = \u001b[43mlm_backbone\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     torch.cuda.synchronize()\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\source\\Anton\\anton\\.venv\\Lib\\site-packages\\peft\\peft_model.py:1875\u001b[39m, in \u001b[36mPeftModelForCausalLM.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1873\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m   1874\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1875\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1876\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1877\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.base_model.generate(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\source\\Anton\\anton\\.venv\\Lib\\site-packages\\trl\\models\\modeling_value_head.py:199\u001b[39m, in \u001b[36mAutoModelForCausalLMWithValueHead.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    188\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03m    A simple wrapper around the `generate` method of the wrapped model.\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03m    Please refer to the [`generate`](https://huggingface.co/docs/transformers/internal/generation_utils)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    197\u001b[39m \u001b[33;03m            Keyword arguments passed to the `generate` method of the wrapped model.\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpretrained_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\source\\Anton\\anton\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\source\\Anton\\anton\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2465\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2457\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2458\u001b[39m         input_ids=input_ids,\n\u001b[32m   2459\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2460\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2461\u001b[39m         **model_kwargs,\n\u001b[32m   2462\u001b[39m     )\n\u001b[32m   2464\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2465\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2470\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2472\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2476\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\source\\Anton\\anton\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:3476\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3474\u001b[39m     probs = nn.functional.softmax(next_token_scores, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m   3475\u001b[39m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3476\u001b[39m     next_tokens = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m.squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m   3477\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3478\u001b[39m     next_tokens = torch.argmax(next_token_scores, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "ppo_trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_FT_Kernel",
   "language": "python",
   "name": "llm_ft_kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
